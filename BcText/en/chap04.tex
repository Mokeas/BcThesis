\chapter{NLG approaches}\label{chap:approaches}
So far we have covered what individual tasks we need to cover in our solution without suggesting a proper structure, which organises these tasks together and creates a compact implementation. There are three main approaches:
\begin{itemize}
	\item Modular architecture - The basic idea is to split the program into modules that handle the NLG problem task-by-task since the tasks are divided reasonably clearly. These modules are then linked with a pipeline. 
	\item Planning approach - The text is generated by applying the right sequence of actions, which consist of both information - conditions under which you can apply them as well as effects they will have on the current context.
	\item Data-driven approach - The tasks are fully deconstructed and the problem is solved globally using statistical methods (e.g. machine learning) focused on the relationship between input and output, but not on individual tasks. 
\end{itemize}
Note that the approaches do not exclude each other. It is possible to use modular architecture while approaching one or more tasks stochastically or to take the global approach and avoid data-driven methods completely. Also grouping more tasks together and approaching them differently is a possibility. This section is about describing those general architectures in detail.

\section{Modular architecture}
This approach was described by \cite{reiter1997building} and became almost a standard for a long time. However, nowadays stochastic and data-driven methods are prioritised over this approach (further discussed later in Chapter). The idea is to construct a module for each NLG task and link those modules via a one-way pipeline. Hence, output of content determination is an input for discourse planning and so forth. But some of the tasks are closely related and therefore it is efficient to assign those tasks to one module. Accustomed layout by \cite{reiter1997building} of modules is: (TODO - A - obrázek schématu)
\begin{itemize}
	\item Text (document) planner $\rightarrow$ content determination + discourse planning
	
	= \emph{“what to say”}
	\item Sentence planner $\rightarrow$ sentence aggregation + lexicalization + REG
	
	= \emph{“how to say it”}
	\item Linguistic realiser
	
	= \emph{“saying it correctly”}
\end{itemize}

Text planner determines the content of the text as well as the order, in which we present the content to the reader. This part of the NLG process is also referred to as strategic generation. Sentence planner transforms messages from text planner to a lexicalized expression. And finally linguistic realiser combines these expressions with regard to syntactic and grammatical rules of given language. Choices made during sentence planning and linguistic realisation are often together called, on the other hand, tactical. Clear distinction between tactical and strategic will be more important later when discussing data-driven methods.
The main advantage of this 3-module is the structure itself, which is easy to follow and understand. In addition, offering clear division of what is each module’s obligations as well as what issue they don’t resolve. Simplicity results in accessible and well-structured code. Moreover, it is easier to change a minor functionality of the program since classifying where to change the code is intuitive. The same logic applies for upgrades - one or more tasks can be approached completely differently than the rest of the code using other methods (e.g. statistical) by keeping the input and output structure.

The core drawback of the approach arises from the same aspect which was mentioned above as an advantage - well defined clear systematic structure. This brings certain limitations. Once we decide sentence order and content we have no chance of changing it later and so the choices in the early stages may later result in an unsolvable issue. Imagine generating a sentence with a limited maximum number of characters. In a text planner we have chosen the content of the sentence, but even when we do every possible combination of lexicalization of the sentence the number of characters still exceeds the upper bound. So what now?! The solution would be to retroactively change the content of the sentence and drop part of the initial information, but this is not possible since the pipeline is one sided. Of course this problem could be bypassed by making the pipeline go backwards, but this would completely break the point of modular approach. The clear line of division among modules would disappear and modules' functionality and objectives would suddenly overlap. To put it simply, assembling the structure of the text before knowing linguistic resources may result in incorrect, ambiguous or bizzare expressions. Therefore other approaches are usually based on breaking the structure and skipping to different tasks depending on the state of the development and the constraints that arise along the process.


\section{Planning approach}
In order to produce a text in a decent quality many decisions must be made resulting in various alternatives. Similarly as described in \cite{fikes1971strips} where the idea is to find a universal robot solver for the world model represented as first-order predicate formulas. The broadness and vagueness of the process of starting with various preconditions and getting to the desired result lead to a another approach, which highly differs from previous two mentioned approaches - planning.

Planning problem (as well as planning approach in general) is described by \cite{gatt2018survey} as “the process of identifying a sequence of one or more actions to satisfy a particular goal”. Actions are then described as preconditions and their effect after applying them. In the terms of NLG, the goal is to convey the message along with its aim (persuade, inform, captive, …) to the target person. The actions then have constraints under which they can be performed and the effect is the change to the current context all leading to the desired goal of the language. The main idea is to create formalism that does not rely on a strict structure of the NLG system and available solutions to alternate between different NLG tasks with the recognition of both current state and effects of the chosen actions all in order to create the best possible language and broaden the limits of pure modular approach. 

\section{Data-driven approach}
Unlike both mentioned approaches above, data-driven methods do not define the architecture of the NLG process. Modular approach states three-moduled architecture connected via one-way pipeline, whereas the main idea behind planning is not having predetermined architecture at all and approaching the process globally with a well-build formalism enabling freedom for combinating actions in. Data-driven methods can be applied regardless of the choice of the architecture. Meaning that stochastic methods can be used in a global approach, single NLG task or certain bigger subsections like strategic or tactical parts, for instance.

As the name implies, data-driven methods crucially rely on data, which consist of inputs and corresponding outputs. Using statistical or probabilistic principles of comparing our current state of the NLG process to a similar state in the data ensures making choices similar to those in the data. Note this definition data can be grasped as a corpus. However, corpus is restricted to only using the end-points of the NLG process - the initial data and the final result output text. Since these methods can be applied on even smaller segments of the generation data can contain processed input or output data in various internal representations, which appear during the process and not just the initial and final stage. For example, when computing sentence planner inputs of our testing data are preverbal messages and outputs are lexicalized texts.

The very first obstacle that arises when performing these methods is the acquisition of the input-output data, because the data must fulfil some requirement. The amount of the dataset should be big enough to ensure the validity and overall principal of the statistical and probabilistic approach. The dataset must not only satisfy the requirement on the quantity, but certain variety must be ensured as monotonic data tend to give one-sided and misleading results. 

Acquiring data itself can be tricky. Easiest scenario when an already established corpus for a specific domain (e.g. weather forecasting, hotel and restaurant recommendation, sport reports and more) exists. These corpora are well-built, but on the other hand useful only when working with the same domain. The reason for that these corpora may be available is firstly because their usage is common and secondly for their input data simplicity. Working with a less-common domain with larger range, types or complexity of data will result probably in the absence of a viable corpus and therefore building the corpus is another problem appended to the NLG process. To overcome this problem we can either build a new corpus from scratch or exploit more stochastic methods that automatically align input with outputs. Then again one disadvantage is that the data are heavily domain-specific. The alignment of the input to the segments of output is crucial for most of the methods except deep neural networks and other machine-learning methods. These methods are recently becoming dominant in certain subfields of NLG such as image-to-text generation.

After assuming we have acquired data for the full-ranged NLG process, we can classify a stochastic approach based on the overall architecture. One group approaches the problem globally and completely decomposes the modular approach in order to both avoid error propagation and allow the software to make decisions freely across multiple tasks and different stages of generation. In opposition, the second group uphold at least the division between tactical and strategic choices.  

As mentioned in the beginning of this section, the data-driven methods can be applied only to process on of the NLG task and not a whole generation in general. So far we have described how these data-driven methods work in general without specifying these methods in detail. We state some examples to further illustrate usage of data-driven approach with different specific methods and range of NLG process they cover:
\begin{enumerate}
	\item \textbf{stochastic process} - Work of \cite{ratnaparkhi2000trainable} is a nice first example to introduce data-driven methods, because he described three systems (NLG1-3) along with their comparison. System are used for tactical generation (semantic context is provided in a corpus given attribute-value pairs aligned with textual outputs) in a air travel domain. First system (NLG1), for given attributes simply chooses a template with the highest number of occurrences in the training data. Second system uses maximum entropy probabilistic model to predict the best proceeding  word taking the already generated and also the attributes yet to be generated into account. However, the dependency of the words in language may not come from their order. Therefore NLG3 predicts the best words based on their syntactic relations represented by a tree. Along with the systems description \cite{ratnaparkhi2000trainable} offers their results in terms of correctness. Both NLG2 and NLG3 heavily outperformed NLG1. Furthermore, NLG3 was performing slightly more accurate than NLG2.  \cite{ratnaparkhi2000trainable} states that both NLG2 and NLG3 can be used in other domains as well, but the complexity of the domain must be somewhat similar to the air travel (meaning quite low). Implicitly domain annotated data must be provided to further exploit NLG2/3 systems.\label{dd-1}
	\item \textbf{classification} - \cite{duboue2003statistical} used classification process in their system for automatic content determination illustrated on biography generation problem. System is provided with initial data and target texts. Algorithm starts with clustering the semantic data (e.g. by age) and matching segments of the output to the pieces of input. This forms a solid base for the process of creating content selection rules using the binary classifier for each attribute of input data whether the attribute should be mentioned or not.\label{dd-2}
	\item \textbf{optimisation} - The article of \cite{marciniak2005beyond} researches the optimisation process in Natural Language Processing (NLP) approached as a integer linear programming problem. They use this approach even in a field of NLG when generating textual route directions. The global approach of results in the elimination of error propagation and has better overall results as a consequence according to \cite{marciniak2005beyond}. Summary of the specifics such as the metric that should be minimized during linear programming is further described, for instance, in \cite{gatt2018survey}).\label{dd-3}
	\item \textbf{probabilistic context-free grammar (PCFG) and parsing} - The idea of mixing the NLG tasks together is further exploit by \cite{konstas2013global} as they call the resolvement of every single task separately "greedy". The need for domain-specific approach is here eliminated as this work is concerned with concept-to-text generation. They use input to model a PCFG and then using the stochastic methods to acquire the best word sentence satisfying the grammar. Such a process can be viewed as an opposite to semantic parsing. This system was tested on three different domains - sportscasting, weather forecasting and air travel query generation. Performance results were described as same or even better to the methods known at the time.\label{dd-4}
\end{enumerate}
Note that these four examples not only show various specific statistical methods, but they illustrate other niances as well. Take the range of the process they cover for instance. Examples (\ref{dd-4}) and (\ref{dd-3}) cover end-to-end process, contrastingly example (\ref{dd-2}) covers only the tactical part and example (\ref{dd-1}) only covers content determination. Moreover (\ref{dd-1}) and (\ref{dd-2}) keep the strategic and tactical division unlike (\ref{dd-4}) and (\ref{dd-3}). Domains differ as well, especially in example (\ref{dd-4}), in which domain does not have to be specified.

These examples share two more similarities except the implicit statistical approach. Firstly, they rely heavily on the testing data and especially their alignment of input and output. Secondly, their results are somewhat superior to other hand-engineered systems. Often hand-crafting a NLG system is relies heavily on the domain and lacks portabiltity and certain variabilitly of the output (respectively achieve the variability by hand is the more tideous job the more variable the output should be). NLG systems grounded on statistics are robust and a task of data acquirement is added to the end-to-end solution. This is counterbalanced by overall better results and more portability as example (\ref{dd-4}) is domain-independent. These methods along with the solid linguistic foundation are nowadays dominant since the robustness is not big enough to be uncomputable with modern computers and the availability (or the opportunity to compute the) and amount of testing data is much better. 