\chapter{A- Requirements analysis}\label{chap:requirements}

When developing a text-producing software, we need to analyse four fundamental aspects:

\begin{itemize}
\item Input data
\item Expected output
\item Goal of the language 
\item Target audience  
\end{itemize}

As we mentioned above using various examples these factors can greatly influence methods and our overall approach to the problem. For instance, producing a routine text with formal requirements will be approached differently than generating an eye-catching book teaser. Communicating these specifics can be tricky when only one side has an insight into computational linguistics. The most suggested method\footnote{This method is useful even when developing software independently without any specific party, that would require the specifics, meaning we can create a corpus to our liking.} is to create a corpus, which is a collection of inputs and corresponding text outputs. This process ensures clear definition of expectations of what the output will look like and prevent proceeding missunderstanding. 

\section{A-Corpora building}
Building a corpus should be supervised and consulted with a domain expert (e.g. for creating medical reports the doctor should participate in creating example texts) in order to achieve the best result possible. For corresponding input an example output should be written by a domain expert. Some NLG problems can find better results when extracting output texts from already existing “approved” texts. For example, when generating a short weather forecast it would be ineffective to create newly-written texts. Much better approach would be to extract forecasts from the most popular weather websites and take those as expected outputs. Benefits of this approach are reduced time spent acquiring example outputs and also ensured quality of the text since using popular websites. Some downsides could arise when applying this approach - acquiring this data can contradict with copyright law and also the output text of our new implemented NLG system (when done optimally) will produce the same texts as those popular websites and therefore there is no reason why our forecast should become more read and popular.

What should a good corpus look like? Corpus should be comprehensive and offer a wide range of pairs input-output. Edge cases, exceptions or less unusual texts should be incorporated in the corpus as well as average text to produce. Corpus should be exhaustive in terms of expectations - once the corpus is agreed upon and finished, adding functionality explicitly is very complicated and is likely to change the overall structure (depending on its complexity). The result of the process above is called initial corpus.

The developer should now make a revision of the initial corpus to guarantee that the NLG system can deliver the expected text, because not every input and corresponding output must necessarily be correct. Firstly, the output text can be improved (e.g. when acquiring texts from existing one). Secondly, the information that is contained in the sentence may not be present nor computable from the given input data\footnote{The information can also be somehow contained, but building tools for its extraction would be insanely time consuming or the time complexity of the extraction would be high and therefore not possible - this is up to the developer to analyse and determine data transformations within his reach.}. This is a critical part of the development as there is no way to resolve this problem every time and it is highly application-dependent. Common solutions are to extend the input data or remove unavailable information from the text and create a new version avoiding the information. Similarly, a compromise of finding hand-formed rules when to convey the information may be the solution. After all the necessary changes to the initial corpus were made, the corpus is now composed strictly of well-built and agreed upon example texts, where every information needed to its generation is contained in a data directly or it can be computed from given data. This finalised corpus is called target text corpus.

To summarise, building a corpus is not obligatory, but highly recommended practice to ease the process of developing the NLG system, especially finding requirements. Precise target primarily precedes the problem that the quality of the developed system is insufficient as the user knows exactly what the output will look like and match his expectations perfectly. In addition, the corpus is a fine tool even for the developer himself. Analysis of each record of the corpus results in an outline of the challenges to overcome and gives a basic idea how the particular NLG problem should be approached.
