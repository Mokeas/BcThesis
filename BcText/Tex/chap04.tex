\chapter{Process of developing an NLG system}\label{chap:process}

The process of developing a text-producing software is complex and requires organised preparation in order to deliver a quality result. This paper so far covered separate segments of the NLG accompanied by numerous examples to illustrate concrete problems. Now we suggest how to order key parts of the development:
\begin{enumerate}
	\item Requirements analysis \label{p-1}
	\item Corpus building \label{p-2}
	\item Choosing suitable architecture and approaches \label{p-3}
	\item Implementation of the NLG system \label{p-4}
	\item Evaluation \label{p-5}
\end{enumerate}

Steps (\ref{p-1}), (\ref{p-2}) and (\ref{p-5}) are further discussed in separate sections below. Steps (\ref{p-3}) and (\ref{p-4}) were already described in detail in \autoref{chap:tasks} and \autoref{chap:approaches}. Both steps (\ref{p-2}) and (\ref{p-5}) are skippable and not required, but their implementation can further enrich the final solution.

\section{Requirements analysis}

First step of developing the NLG system is to carefully analyse fundamental aspects that will establish the approach and overall functioning of the program. We state six such aspects:
\begin{itemize}
	\item Input data
	\item Expected output
	\item Target language
	\item Goal of the language 
	\item Target audience
	\item Usage of already existing tools  
\end{itemize}

Examples in the article were created to demonstrate the various effects these aspects could have. This section will briefly summarise nuances that can occur. 

\textbf{Input} data can have different inner representations: table, database, graphs, images, etc. Furthermore data can be in its initial state and probably will require some preprocessing to, for instance, extract computable information or transform data into better structured and more suitable other representation. Some representations can result in creating non-trivial tools to extract needed information for the generation, e.g. image requires image recognition software.

\textbf{Expected output} can be specified in corpus (described below). Knowledge of desired output is key to analyse the full extent of other aspects.

\textbf{Target language} can as well influence the approach greatly, especially the solution for the tactical part of the program. In the section about linguistic realisation (\autoref{chap:tasks}) we have covered some differences between languages. Naturally, deeper linguistic analysis is required to fully recognize the specifics of the language and how to approach lexicalization, REG and realisation. Moreover, the NLG system can generate identical messages in multiple different languages and then the approach for the solution can be restricted to only those that strictly divide tactical and strategic choices.

\textbf{Goal of the language} is very important as seen in examples throughout the article. For instance, producing a routine text with formal requirements will be approached differently than generating an eye-catching book teaser.

\textbf{Target audience} has a couple of characteristics: age, literacy rate, level of education or knowledge of the domain and topic. These details should be taken into consideration in order to deliver the message to the reader in the most convenient and understandable way possible. 

Lastly, a great number of \textbf{already existing tools} has been developed and it is a possibility to incorporate these tools into newly constructed solution. Examples can be domain specific corpora or specific language realisers. Utilising these already established resources can upgrade the solution. However, the negative impact of such usage can be making ponderous transformations to create input in correct form that is required by the incorporated system as well as accommodating the approach to the tool.
 
\section{Corpora building}
As we mentioned above, requirement factors can greatly influence methods and our overall approach to the problem. Communicating these specifics can be tricky when only one side has an insight into computational linguistics. The most suggested method is to create a corpus, which is a collection of inputs and corresponding text outputs. This process ensures a clear definition of expectations of what the output will look like and prevents misunderstanding. This method is useful even when developing software independently without any specific party, that would require any specifics. Corpus is then built to our own expectations creating a solid base for the process of approach analysis and implementation of the NLG system in general. 

Additionally, corpus is necessary when applying statistical methods since they rely completely on the content of the corpus and the correlation in-between. Lastly, corpus does not have to be composed of end-to-end NLG pairs, but rather contain inputs and corresponding outputs for a smaller part of the NLG (one or more tasks) in order to create a foundation for stochastic methods to resolve the problem. Inputs and outputs can then vary greatly in the inner representation as for instance inputs for content determination and REG will surely differ. What is more, since data structures  alter even for the same NLG task solution the corpus should be built (or transformed) so that the input and output fit the specific solution's representations.

Hand-building a corpus from scratch should be supervised and consulted with a domain expert (e.g. for creating medical reports the doctor should participate in creating example texts) in order to achieve the best result possible. For corresponding input an example output should be written by a domain expert. The result of this process is called an initial corpus. The developer should now make a revision of the initial corpus to guarantee that the NLG system can deliver the expected text, because not every input and corresponding output must necessarily be feasible. Firstly, the output text can be improved (e.g. when acquiring texts from existing one). Secondly, the information that is contained in the sentence may not be present nor computable from the given input data.\footnote{The information can also be somehow contained in the initial data, but building tools for its extraction would be insanely time consuming or the time complexity of the extraction would be high and therefore not possible: this is up to the developer to analyse and determine data transformations within his reach.} This is a critical part of the development as there is no way to resolve this problem every time and it is highly application-dependent. Common solutions are to extend the input data or remove unavailable information from the text and create a new version avoiding the information. Similarly, a compromise of finding hand-formed rules when to convey the information may be the solution. After all the necessary changes to the initial corpus were made, the corpus is now composed strictly of well-built and agreed upon example texts, where every information needed to its generation is contained in a data directly or it can be computed from given data. This finalised corpus is called target text corpus.

What should a good corpus look like? Corpus should be comprehensive and offer a wide range of pairs input-output. Edge cases, exceptions or less unusual texts should be incorporated in the corpus as well as average text to produce. Naturally, the number of normal texts to produce should be significantly higher than the number of rare examples to indicate the correct ratio. Corpus should be exhaustive in terms of expectations: once the corpus is finished, adding functionality explicitly is very complicated and is likely to change the overall structure, depending on its complexity. 

Described process of hand-building corpus is highly labour-expensive and therefore usually domain-dependent systems for automatic acquisition of the data are developed lately further exploiting the strengths of statistical methods in combination with the amount of accessible data. Some NLG problems can find better results when extracting output texts from already existing “approved” texts. For example, when generating a short weather forecast it would be ineffective to create newly-written texts. Much better approach would be to extract forecasts from the most popular weather websites and take those as expected outputs. Benefits of this approach are reduced time spent acquiring example outputs and also ensured quality of the text since using popular websites. Some downsides could arise when applying this approach: acquiring this data can contradict with copyright law and range of the input data can differ resulting in unavailable knowledge. Also the output text of our new implemented NLG system (when done optimally) will produce similar texts as those popular websites and therefore there is no reason why our forecast should become more read and popular than the already existing ones. This approach is suitable when the domain content will probably be very similar, reducing the amount of knowledge that may be unavailable. On top of that, the number of available output texts must be high to make use of the stochastic approach. Such domains are weather forecasts, sport reports or the (air) travel domain and more.

To summarise, building a corpus is not obligatory, but highly recommended practice to ease the process of developing the NLG system, especially finding requirements. Precise target corpus primarily precedes the problem that the quality of the developed system is insufficient as the user knows exactly what the output will look like and match his expectations perfectly. When applying statistical methods, a well-defined and comprehensive corpus becomes a necessity since the stochastic approach relies crucially on the power of data. Lastly, the corpus is a fine tool even for the developer himself: analysis of each record of the corpus results in an outline of the challenges to overcome and gives a basic idea how the particular NLG problem should be approached.

\section{Evaluation}
After NLG yields the textual result an intuitive action would be to rate it and then reflect the approach and implementation based on the rating. From a practical point of view this step can be processed by a user or client who expects a certain quality and decides whether the quality is matched or not. From an academic point of view, this task is about deciding how to measure what is the "optimal" output as described in footnote \ref{footnote-opt}. Computer science problems usually contain a metric, which easily decides what solution is better. For instance, graph problems usually have an optimal solution to the problem and algorithms are compared based on time complexity of finding this optimal solution.\footnote{Exceptions are probabilistic and approximation algorithms that find acceptable (compared to the optimal) solution in a faster time} However, in linguistics the comparison metric is not obvious, but rather complicated.

This can be demonstrated in a school-leaving essay. Deciding about the grade will likely be uniform since the base is some general criteria that have to be matched. However, how to decide what work was the best (if the goal was to pick the best essay). Surely, we can restrict ourselves to only the theses that were graded with the best grade, but how to choose among them? To summarise, in linguistics it is trivial for a human to decide if the result is "good" or "bad", but it is difficult to pick the best among the "good" ones. "Bad" results can be grammatically incorrect, factually wrong or the communicative goal was not achieved. This process is even harder for computers and therefore a human element is often required to rate the results. 

It seems that the measure tools differ very much from what to measure to how to to measure it. Readability, fluency, and correctness can be viable attributes rating the linguistic appearance of the text. What information is present in the text, how it is presented to the reader, whether the information is relevant and if the communicative goal was achieved are attributes to grade content of the text and the overall vibe. Grading these attributes can be then approached alternatively as well: by grading on discrete or continuous scale, ordering texts or simply picking the best text in a given aspect. The metric should be designed to reflect important aspects for the given texts.

The uncertainty of this step is predictable as the importance and consequences of small nuances in the texts are interpreted subjectively. \cite{gatt2018survey} recommend aiming for diversity in the approach to yield a wide range of results and then decide the best final approach based on not only the results, but on the correlation between them as well.

