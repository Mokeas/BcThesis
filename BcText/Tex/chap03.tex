\chapter{NLG approaches}\label{chap:approaches}
We will divide approaches based on two aspects:
\begin{itemize}
	\item Structure of solution
	\item Methods used in a solution
\end{itemize}	

Note that we can fuse structure and methods arbitrarily in our desired solution. 

\section{Structure of solution}
So far we have covered what individual tasks we need to cover in our solution without suggesting a proper structure, which organises these tasks together and creates a compact implementation. There are two main approaches:
\begin{itemize}
	\item \textbf{Modular architecture} -- This architecture uses a pipeline of modules. Often this architecture follows the design suggested by \cite{reiter1997building}, with each module corresponding to one or more tasks they suggest (see \ref{chap:tasks}). 
	\item \textbf{Global approach} -- The tasks are fully deconstructed and the problem is solved globally in order to get rid of the limitations associated with modular architecture. 
\end{itemize}

\subsection{Modular architecture}\label{section:modules}
This approach was described by \cite{reiter1997building} and became almost a standard for a long time. The idea is to construct a module for each NLG task and link those modules via a one-way pipeline. Hence, output of content determination is an input for discourse planning and so forth. But some of the tasks are closely related and therefore it is efficient to assign those tasks to one module. Accustomed layout by \cite{reiter1997building} of modules is: 
\begin{itemize}
	\item Text (document) planner -- content determination + discourse planning
	
	= \emph{``what to say”}
	\item Sentence planner -- sentence aggregation + lexicalization + REG
	
	= \emph{``how to say it”}
	\item Linguistic realiser
	
	= \emph{``saying it correctly”}
\end{itemize}

Text planner determines the content of the text as well as the order, in which we present the content to the reader. This part of the NLG process is also referred to as strategic generation. Sentence planner transforms messages from text planner to a lexicalized expression. And finally linguistic realiser combines these expressions with regard to syntactic and grammatical rules of given language. Choices made during sentence planning and linguistic realisation are often together called, on the other hand, tactical. Clear distinction between tactical and strategic will be more important later when discussing data-driven methods.

The main advantage of this 3-module is the structure itself, which is easy to follow and understand. In addition, offering clear division of what is each module’s obligations as well as what issues they do not resolve. Simplicity results in accessible and well-structured code. Moreover, it is easier to change a minor functionality of the program since classifying where to change the code is intuitive. The same logic applies for methods used across the solution: one or more tasks can be approached completely differently than the rest of the code using other methods (e.g., statistical) while keeping the input and output formats of each module.

However, the strict sequentiality of the architecture is also its main drawback. Once we decide sentence order and content we have no chance of changing it later and so the choices in the early stages may later result in an unsolvable issue. Imagine generating a sentence with a limited maximum number of characters. In a text planner we have chosen the content of the sentence, but even when we do every possible combination of lexicalization of the sentence the number of characters still exceeds the upper bound. So what now?! The solution would be to retroactively change the content of the sentence and drop part of the initial information, but this is not possible since the pipeline is one sided. Of course this problem could be bypassed by making the pipeline go backwards, but this would completely break the principle of modular approach. The clear line of division among modules would disappear and modules' functionality and objectives would suddenly overlap. 

\subsection{Global approach}
Assembling the structure of the text before knowing linguistic resources may result in incorrect, ambiguous or bizarre expressions. Therefore global approach is based on breaking the crisp division of the modular structure and alternating between different NLG tasks depending on the state of the development and the constraints that arise along the process while having full context available at all times. This is the core idea of the approach, however, how to implement similarly working solution differ hugely. One example of such implementation, that can be formally described, is planning.

\subsection{Planning approach}
In order to produce a text in a decent quality many decisions must be made resulting in various alternatives. Similarly as described in \cite{fikes1971strips} where the idea is to find a universal robot solver for the world model represented as first-order predicate formulas. The broadness and vagueness of the process of starting with various preconditions and getting to the desired result lead to planning.

Planning problem (as well as planning approach in general) is described by \cite{gatt2018survey} as ``the process of identifying a sequence of one or more actions to satisfy a particular goal”. Actions are then described as preconditions and their effect after applying them. In the terms of NLG, the goal is to convey the message along with its aim (persuade, inform, captive, …) to the target person. The actions then have constraints under which they can be performed and the effect is the change to the current context all leading to the desired goal of the language. The main idea is to create formalism that does not rely on a strict structure of the NLG system and available solutions to alternate between different NLG tasks with the recognition of both current state and effects of the chosen actions all in order to create the best possible language and broaden the limits of pure modular approach. 

\section{Methods}
After we specified how to compose a solution, we propose methods and approaches to use in a chosen structure of the solution. In \ref{chap:tasks} we proposed  methods mostly using structures and formalisms (e.g., templates or grammars) that are based on the linguistic knowledge of the language. However, even in \ref{chap:tasks} we mentioned data-driven methods that can be incorporated to solve a particular NLG task. Data-driven (or statistical,  stochastic) methods are a widely discussed topic among the NLG community as their popularity grows and their results are recently getting better, often outperforming more traditional procedures. Therefore the whole next subsection is dedicated to them. 

\subsection{Data-driven approach}
As the name implies, data-driven methods crucially rely on data, which consist of inputs and corresponding outputs. Using statistical or probabilistic principles of comparing our current state of the NLG process to a similar state in the data ensures making choices similar to those in the training dataset. In the field of linguistics this data is referred to as corpus. Since these methods can be applied on even smaller segments of the generation data can contain processed input or output data in various internal representations, which appear during the process and not just the initial and final stage. For example, when computing sentence planner inputs of our testing data are preverbal messages and outputs are lexicalized texts.

The very first obstacle that arises when performing these methods is the acquisition of the input-output data, because the data must fulfil some requirement. The amount of the dataset should be big enough to ensure the validity and overall principal of the statistical and probabilistic approach. The dataset must not only satisfy the requirement on the quantity, but certain variety must be ensured as monotonic data tend to give one-sided and misleading results. 

Acquiring data itself can be tricky. Easiest scenario when an already established corpus for a specific domain (e.g., weather forecasting, hotel and restaurant recommendation, sport reports and more) exists. These corpora are well-built, but on the other hand useful only when working with the same domain. The reason for that these corpora may be available is firstly because their usage is common and secondly for their input data simplicity. Working with a less-common domain with larger range, types or complexity of data will result probably in the absence of a viable corpus and therefore building the corpus is another problem appended to the NLG process. To overcome this problem we can either build a new corpus from scratch or exploit more stochastic methods that automatically align input with outputs. Then again one disadvantage is that the data are heavily domain-specific. The alignment of the input to the segments of output is crucial for most of the methods except deep neural networks and other machine-learning methods. These methods are recently becoming dominant in certain subfields of NLG such as image-to-text generation.

After assuming we have acquired data for the full-ranged NLG process, we can classify a stochastic approach based on the overall architecture. One group approaches the problem globally and completely decomposes the modular approach in order to both avoid error propagation and allow the software to make decisions freely across multiple tasks and different stages of generation. In opposition, the second group upholds at least the division between tactical and strategic choices.  

We state some examples to further illustrate usage of data-driven approach with different specific methods and range of NLG process they cover:
\begin{enumerate}
	\item \textbf{stochastic process} - Work of \cite{ratnaparkhi2000trainable} is a nice first example to introduce data-driven methods, because he described three systems (NLG1-3) along with their comparison. Systems are used for tactical generation (semantic context is provided in a corpus given attribute-value pairs aligned with textual outputs) in an air travel domain. First system (NLG1), for given attributes, simply chooses a template with the highest number of occurrences in the training data. Second system uses a maximum entropy probabilistic model to predict the best proceeding  word taking the already generated and also the attributes yet to be generated into account. However, the dependency of the words in language may not come from their order. Therefore NLG3 predicts the best words based on their syntactic relations represented by a tree. Along with the system's description \cite{ratnaparkhi2000trainable} offers their results in terms of correctness. Both NLG2 and NLG3 heavily outperformed NLG1. Furthermore, NLG3 was performing slightly more accurately than NLG2.  \cite{ratnaparkhi2000trainable} states that both NLG2 and NLG3 can be used in other domains as well, but the complexity of the domain must be somewhat similar to the air travel (meaning quite low). Implicitly domain annotated data must be provided to further exploit NLG2/3 systems.\label{dd-1}
	\item \textbf{classification} - \cite{duboue2003statistical} used classification process in their system for automatic content determination illustrated on biography generation problem. System is provided with initial data and target texts. Algorithm starts with clustering the semantic data (e.g., by age) and matching segments of the output to the pieces of input. This forms a solid base for the process of creating content selection rules using the binary classifier for each attribute of input data whether the attribute should be mentioned or not.\label{dd-2}
	\item \textbf{optimisation} - The article of \cite{marciniak2005beyond} researches the optimisation process in Natural Language Processing (NLP) approached as an integer linear programming problem. They use this approach even in a field of NLG when generating textual route directions. The global approach results in the elimination of error propagation and has better overall results as a consequence according to \cite{marciniak2005beyond}. Summary of the specifics such as the metric that should be minimized during linear programming is further described, for instance, in \cite{gatt2018survey}).\label{dd-3}
	\item \textbf{probabilistic context-free grammar (PCFG) and parsing} - The idea of mixing the NLG tasks together is further exploit by \cite{konstas2013global} as they call the division of every single task separately ``greedy". The need for domain-specific approach is here eliminated as this work is concerned with concept-to-text generation. They use input to model a PCFG and then use the stochastic methods to acquire the best word sentence satisfying the grammar. Such a process can be viewed as an opposite to semantic parsing. This system was tested on three different domains: sportscasting, weather forecasting and air travel query generation. Performance results were described as same or even better to the methods known at the time.\label{dd-4}
\end{enumerate}
Note that these four examples not only show various specific statistical methods, but they illustrate other nuances as well. Take the range of the process they cover for instance. Examples (\ref{dd-4}) and (\ref{dd-3}) cover end-to-end process, contrastingly example (\ref{dd-2}) covers only the tactical part and example (\ref{dd-1}) only covers content determination. Moreover (\ref{dd-1}) and (\ref{dd-2}) keep the strategic and tactical division unlike (\ref{dd-4}) and (\ref{dd-3}). Domains differ as well, especially in example (\ref{dd-4}), in which domain does not have to be specified.

These examples share two more similarities except the implicit statistical approach. Firstly, they rely heavily on the testing data and especially their alignment of input and output. Secondly, their results are somewhat superior to other hand-engineered systems. Often hand-crafting a NLG system relies heavily on the domain and lacks portability and certain variability of the output (respectively achieving the variability by hand is the more tedious job the more variable the output should be). NLG systems grounded on statistics are robust and a task of data acquisition is added to the end-to-end solution. This is counterbalanced by good (or even better results as described, for instance, in \citet{konstas2013global} more portability as example (\ref{dd-4}) is domain-independent. These methods along with the solid linguistic foundation are nowadays dominant \citep{gatt2018survey} since the robustness is not big enough to be uncomputable with modern computers and the availability  and amount of testing data is much higher. 